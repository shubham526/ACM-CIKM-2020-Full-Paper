%
% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf,authordraft]{acmart}
\usepackage{booktabs}
\usepackage{verbatim}
\usepackage{graphicx}

 %activate todo's
\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}\PackageWarning{TODO:}{#1!}}
% deactivate todos
%\newcommand{\todo}[1]{ \PackageWarning{TODO:}{#1!}}

%
% defining the \BibTeX command - from Oren Patashnik's original BibTeX documentation.
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08emT\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
% Rights management information. 
% This information is sent to you when you complete the rights form.
% These commands have SAMPLE values in them; it is your responsibility as an author to replace the commands and values with those provided to you when you complete the rights form.
%
% These commands are for a PROCEEDINGS abstract or paper.
\copyrightyear{2018}
\acmYear{2018}
\setcopyright{acmlicensed}
\acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection, June 03--05, 2018, Woodstock, NY}
\acmPrice{15.00}
\acmDOI{10.1145/1122445.1122456}
\acmISBN{978-1-4503-9999-9/18/06}

%
% These commands are for a JOURNAL article.
%\setcopyright{acmcopyright}
%\acmJournal{TOG}
%\acmYear{2018}\acmVolume{37}\acmNumber{4}\acmArticle{111}\acmMonth{8}
%\acmDOI{10.1145/1122445.1122456}

%
% Submission ID. 
% Use this when submitting an article to a sponsored event. You'll receive a unique submission ID from the organizers
% of the event, and this ID should be used as the parameter to this command.
%\acmSubmissionID{123-A56-BU3}

%
% The majority of ACM publications use numbered citations and references. If you are preparing content for an event
% sponsored by ACM SIGGRAPH, you must use the "author year" style of citations and references. Uncommenting
% the next command will enable that style.
%\citestyle{acmauthoryear}

%
% end of the preamble, start of the body of the document source.
\begin{document}

\section{Neural Methods}

\subsection{Logistic Network}\label{jordan-logistic}
We first implement a layer logistic network, using the PyTorch library \cite{pytorch}, and the following features from \cite{nanni2018entity}:

% \begin{itemize}
%     \item Passage-header-bm25
%     \item Passage-content-bm25
%     \item Passage-content-overlap
%     \item Passage-
% \end{itemize}

We train on 88,000 examples pulled from the 2018 Wikipedia dump. 
We use minibatching, batches of 500, and cross entropy as the loss function.

\subsection{ELMo Bi-LSTM}\label{jordan-lstm}
Another feature we use is a Bi-LSTM neural network model for predicting the relevance of an aspect.
We embed the paragraph context of the mention, and the text content of each aspect, by using ELMo vectors \cite{elmo}.
ELMo allows us to embed each individual sentence  --

\subsection{Co-Entity Context}\label{jordan-co-entity}
One valuable source of context for which we can use to link an aspect to an entity mention, is that entities can be mentioned together in the same paragraph.
We can use this context to further evaluate the relevance of aspects, given how much they have in common with the aspects of entities that co-occur in the same passage. 

For example, the Chocolate Wikipedia page contains many sections that cover a broad array of subtopics. One of these sections is the ``Health Benefits of Chocolate'', which is notably smaller than the other sections in comparison. The section contains very specific medical terms that may not occur in a sentence where this section would be the correct aspect to link to. As an example, consider a sentence like ``Make sure that you keep away chocolate from your dogs.''. There is no indication that this has anything to do with health, but if we first linked the aspect of dogs, we would see that the ``Health'' section explicitly mentions the danger of poisoning from chocolate. Using this context, it then becomes clear what the relevant section is in Chocolate: not the section that mentions chocolate the most, but the one that talks about Theobromine poisoning and side effects.
While the mentioned entity in the paragraph context is used to directly link to an aspect section on that entity's page, we can also use the other entities that co-occur with this mention in the same paragraph. By performing aspect linking on these other entities, we can use the resulting aspects, and their content, to develop a model for determining the relevance of the primary mention's aspect.

% To achieve this, we first link co-occuring entities with their most relevant aspects.
% We determine the relevance of the aspects given the Logistic Network outlined previously \ref{jordan-logistic}.
% Each linked aspect contains the text, header, and entity mentions associated with that section.
% To score the aspects of the primary mention, we try the following approaches:
In the following sections, we define \textit{co-entities} as the entities that co-occur with the mentioned entity that we are trying to link.
Each co-entity contains a set of aspects which are Wikipedia sections. 
We refer to these aspects as co-aspects, as they are not the primary aspects we are trying to determine the relevance of for a given mention.

\subsubsection{Distributional Model}
To construct an inference model that depends on the context of co-occuring entities, we consider the following measures:

\textbf{Aspect Link Likelihood.} Our task requires that we choose the aspect that best explains the context of an entity mention. We use a measure of relevance to represent the likelihood that an aspect should be linked to an entity given a mention. As described in section \ref{jordan-logistic}, we use a logistic network for this purpose. 

\textbf{Co-Entity Inference Likelihood}. The presence of co-entities in a paragraph does not necessarily indicate that they are a valuable source of information. For example, a very large paragraph can contain many co-entities, and not all of the co-entities will be contextually related to our entity of interest. Therefore we seek to derive a measure of inference that predicts the likelihood that a co-entity shares the same context as an entity. This determines how much we can infer the relevance of an aspect given information about an entity (such as the entity's title, page links, its co-aspects, etc.).

\textbf{Co-Aspect Inference Likelihood}. One drawback of a Co-Entity Inference Likelihood measure is that it is more of a global measure of contextual similarity. Consider the ``Chocolate'' example in section \ref{jordan-disambig}. Dogs and chocolates are not very related to each other, although we saw a context in which these two entities were related to each other, and that's the fact that chocolate is poisonous to dogs. Both entities have a section discussing this fact, but these sections are far smaller than the other sections in these entities by comparison. In the case of a global measure of inference, we would expect there to be a very weak correlation between chocolate and dogs, as they possess few overlapping topics. If we instead looked at the individual sections in each page, we would see that the ``Health'' section in ``Dog'' frequently mentions the dangers of chocolate and theobromine poisoning. 

Given these measures, we propose the following statistical model:

The 

We construct a set of conditional distributions from co-aspects to aspects.
These distributions represent how relevant each aspect is, given the relevance of a set of co-aspects.
We obtain the relevance of of each aspect by marginalizing the relevance score of each co-aspect over the set of aspects given a conditional distribution.
The goal is then to determine the best combination of conditional distributions that predicts the relevance of aspects given co-aspects.
We represent the set of conditional distributions as a neural network, which call a conditional network.

We investigate the performance of several methods for combining the outputs of these networks:

\textbf{Fixed Logistic Network.} We first train the logistic network and fix its parameters before training the conditional network. We then score the relevance each co-aspect, given a co-mention, using the logistic network. These scores become the input layer for the conditional network, while the output layer of the conditional network represents the scores of the primary aspects, given the initial scores  is a marginalization over the conditional distributions between aspects and co-aspects. We then train the conditional network such that it best predicts the relevance of aspects give fixed output of the logistic network. While this can be considerably faster than other methods, it makes the assumption that the optimal parameters for the logistic network are independent of those of the conditional network.

\textbf{Joint Training.} We jointly train the logistic network and the conditional network using expectation maximization.


\textbf{Pseudorelevance Feedback.} In this example, we model the relevance of aspects by marginalizing over words.



\bibliographystyle{ACM-Reference-Format}
\bibliography{references}
\end{document}