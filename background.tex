\section{Background}
\label{sec:Background}
\label{subsubsec:Lexical and Semantic features} 
\label{entity-aspect-representation}

Nanni et al.\cite{nanni2018entity} suggest a range of the lexical and semantic features. Since our method builds on these features, we briefly describe them here.
%al.\cite{nanni2018entity} consider three types of aspect representations and rank aspects based on similarity of mention in context to:(1) Header, (2) Content and, (3) Entity overlap with each section on the Wikipedia page of the entity. They use five methods to derive features from the aspect representations above: (1) TF-IDF, (2) BM25, (3) Word Embeddings, (4) Entity Embeddings, 

Nanni et al. \cite{nanni2018entity} uses a ``bag of words'' vector space model to represent entity aspects. They consider three different ways of comparing the context of the entity mention based on the following entity aspect fields:


\begin{enumerate}

\item \textbf{Header.} Rank aspects based on similarity of the mention in context to the header of each section in the Wikipedia page.

\item \textbf{Content.} Rank aspects based on the similarity between the mention in context and the content of each section of the Wikipedia page of the entity.

\item \textbf{Entity.} Overlap of entities mentioned in the context of the entity mention and the content of a section on the Wikipedia page of the entity.
\end{enumerate}


They use five methods to derive features from these fields: 

\begin{enumerate}
    \item \textbf{TF-IDF.} Cosine similarity between the TF-IDF (logarithmic, L2-normalized) vector of contextual mention and aspect field.
    \item \textbf{BM25.} Rank aspect representations using the contextual mention as a query using BM25 ($k_1=2, b=0.75)$.
    \item \textbf{Word Embeddings.} Cosine similarity between the mention in context and
the aspect using pre-trained GloVe \cite{pennington2014glove} embeddings of dimension 300. 
    \item \textbf{Entity Embeddings.} Using 500 dimensional RDF2Vec \cite{ristoski2016rdf2vec} embeddings to embed entities in the context of the entity mention and a section from the Wikipedia page of the entity, then compute the document vector using the TF-IDF of an an entity in context of the entity mention and its embedding.
    \item \textbf{Term Overlap.} Number of shared words after tokenization.
    \item \textbf{Size.} a context-independent feature, which is the numbers of words in an entity aspect's text.
\end{enumerate}

Furthermore, they explore different context sizes, sentence, paragraph, and section.


% \textbf{TF-IDF.} Cosine similarity between the TF-IDF (logarithmic, L2-normalized) vector of contextual mention and aspect.

% \textbf{BM25.} Rank aspect representations using the contextual mention as a query using BM25 ($k_1=2, b=0.75)$.

% \textbf{Word Embeddings.} Cosine similarity between the mention in context and
% the aspect using pre-trained GloVe \cite{pennington2014glove} embeddings of dimension 300. 

% \textbf{Entity Embeddings.} Use 500 dimensional RDF2Vec \cite{ristoski2016rdf2vec} embeddings to embed entities in the context of the entity mention and a section from the Wikipedia page of the entity, then compute
% the document vector using the TF-IDF of an an entity in context of the entity mention and its embedding.





%\textbf{entity overlap}. The mention context (the sentence, paragraph, or section that an entity mention occurs in) contains one or more entities. Similarly, the text of each entity aspect also contains entities. We count the number of unique overlapping entities between the mention context an entity aspect.

%\textbf{content overlap.} Similar to the \textbf{entity overlap} feature, we count the number of unique overlapping unigrams between the mention context and the entity aspect's main body of text.

%\textbf{size}. This is simply the number of words contained in an entity aspect's main body of text.

%\textbf{BM25}. The Okapi Best Match 25 (BM25) function is commonly used to rank documents according to relevance to a query. In this paper, the query consists of the mention context (sentence, paragraph, or section), while the documents represent candidate entity aspects. We consider two versions of BM25 dependant on the query field: BM25-header queries the section header of each aspect, while BM25-content queries the main body of text contained in the section that an entity aspect represents.

%\textbf{w-emb. } \ld{third person!} We embed both the mention context and the entity aspect main body of text using Word2Vec. Each embedding consists of the mean embedding of each word contained in the text. The final score is cosine similarity measures between the mention context embedding and the entity aspect embedding. 

