\section{Approach}
\label{sec:Approach}
Our goal is to enrich entity links with fine-grained aspects of the entities. In this work, we consider whether a static measure of entity salience and relatedness which has not been trained for this task can help. We further explore to which extent aspect-to-aspect similarities can further improve the results in a joint-aspect-linking approach that is reminiscent to joint entity linking.  We refer to the entity that we want to link to the correct aspect link as the \emph{target entity}.

\ld{We should prepare the reader in terms of why we are defining features. Maybe with a sentence like this "In the following we discuss several ideas a micro-approaches. In the evaluation we will compare them on their own, an through a supervised combination that uses each as a feature. }


%To study the role of entity salience, entity relatedness and co-occurring entities, we describe below several ranking strategies based on these and later combine them in a learning-to-rank system. In all our discussions, we refer to the entity that we are trying to aspect link as the target entity.

\subsection{Reproduction of Nanni Et Al.}

We reimplemented Nanni et al's features as described in background. We explore three machine methods for combining these features, Learning to Rank, Logisitc Network

Additional we further include a \ld{todo, motivate and describe here} BiLSTM feature. \ld{also explain that we are using elmo}


% \subsection{Neural Methods.}
% We use the PyTorch library \cite{pytorch} to construct our neural networks. There are three neural networks that we consider both independently, and together in a joint model. 

\subsubsection{Entity Aspect Relevance \ld{demoted in level, because there should only be four subsections}}
In this paper we model the relevance of an entity aspect with respect to an entity mention using the features described in Section \ref{entity-aspect-representation}. We learn a linear combination of these features using a simple logistic network, trained with respect to binary cross entropy.

We also consider a second representation of entity aspect relevance using a Bi-LSTM model, wherein we embed each sentence in a segment of text using ELMo \cite{elmo}. ELMos is a contextual language embedding model that takes as input a sentence, and outputs word embeddings that depend upon the context given by the sentence. Each word embedding consists of three vectors, and in this paper we concatenate these vectors together to obtain a final word embedding. We then average all of the word embeddings in a sentence together to obtain the embedding of a sentence. These embeddings are used as sequential inputs in a BiLSTM. We embed both the paragraph context of an entity's mention and the text of each candidate entity aspect. (number of hidden layers to be determined ...)

 
\subsection{Entity Salience for Aspect Linking}
\label{subsec:Entity Salience for Aspect Linking}
Our first extension is to incorporate indicators from entity salience detection.
%\ld{needs a reminder such as "Our first extension is to incorporate indicators from entity salience detection."}

\input{salience-motivation}
We hypothesize that entity salience is a useful indicator of aspects for entities. We use SWAT \cite{swat}  to find the salience of an entity in text. Given some text, SWAT outputs the entities along with their salience scores in the text. For example, using the online demo \footnote{https://swat.d4science.org/}, given the two passages in Figure \ref{fig:Salience}, SWAT correctly predicts \textit{Boris Johnson} as salient in Passage 2 (Score = 0.6), and non-salient in Passage 1 (Score = 0.15). 
We use salience detection in our aspect-linking approach by incorporating the following indicators.

%\ld{Needs a sentence such as "We use salience detection in our aspect-linking approach by incorporating the following indicators."}

\begin{enumerate}
    \item \textbf{Salience of Entity Mention in aspect (Sal-EM).} Score of the aspect is equal to the salience score of the entity mention in the aspect if the entity is salient, and zero otherwise.
    
    \item \textbf{Salient Entities in Context (SEC).} Score an aspect by summing over the salience score of entities $e \in E$, where $E = E_A \cap E_C$, $E_A =$ aspect entities, and $E_C =$ salient entities in sentence, paragraph and section context.
    
    \item \textbf{All Entities in Context (AEC).} 
    To investigate the extent to which salient entities can affect the performance, we also experiment with using both salient and non-salient entities from SWAT for $E_C$ in \ref{subsec:Entity Salience for Aspect Linking}(2) above. 
\end{enumerate}


\subsection{Entity Relatedness for Aspect Linking}
\label{subsec:Entity Relatedness for Aspect Linking}

\ld{Fede was using the entity relatedness measure "RDF2Vec", we replace this with other Entity Relatedness measures like in SWAT. However, we need to explain why we changed this. For example we can say that Rdf2Vec did not work as well, but that is a claim we need to substantiate with evidence. @jordan was playing around with Rdf2vec but did not get good results, maybe we can include some of these 'bad' results to support our point?  @shubham maybe you can get some of the Rdf2Vec results and say a few words about how they differ.  I can imagine that RDF2Vec does not work so well, because it is trained on typed  relations available in DBpedia, but these are mostly about people, organizations, and places. I don't know what SWAT is trained on, but even if it is hyperlinks on Wikipedia, it has a better chance of working in our "open-domain" topic.  --- Of course this is speculation, but if we can substantiate the speculation, it would make this paper even stronger. }


Entity Relatedness is a measure of how strongly related two entities are. For example, consider the entities, \textit{Boris Johnson}, \textit{Theresa May}, and \textit{Donald Trump}. Intuitively, one would say that \textit{Boris Johnson} is more strongly related to \textit{Theresa May} than to \textit{Donald Trump} because both \textit{Boris Johnson} and \textit{Theresa May} are British politicians and had some role in Brexit. We hypothesize that this measure of entity relatedness can help in aspect linking. More concretely, we hypothesize that an aspect mentioning many related entities to the target entity  is a good candidate for an aspect for the given target entity. For example, consider the aspect about \textit{Boris Johnson} pertaining to his role as the \textit{Prime Minister of UK} during the COVID-19 pandemic.

\begin{quote}
    \textbf{Aspect.} Prime Minister of UK \\
    \textbf{Content.}
    Former Prime Minister Theresa May has criticised world leaders for failing "to forge a coherent international response" to the coronavirus pandemic. Mrs May's intervention comes as Boris Johnson and Sir Keir Starmer face each other at Prime Minister's Questions for the first time later. \footnote{https://www.bbc.com/news/uk-politics-52553237}.
\end{quote}
The aspect above may be linked to the entity \textit{Boris Johnson} since it mentions several related entities such as \textit{Theresa May} and \textit{Sir Keir Starmer}. 

Co-occurring entities are entities which co-occur with a given entity in a particular context such as a sentence, passage or article. For example, entities which might co-occur with \textit{Boris Johnson} in a passage may be \textit{Theresa May}, \textit{United Kingdom} and \textit{Brexit}. We study the role of such co-occurring entities on the aspect linking task by comparing whether the frequency or relatedness of these co-occurring entities to the target entity is a better indicator of aspects and under what conditions they work. 


\subsubsection{Co-occurring entities with an entity from context}
\label{subsubsec:Co-occurring entities with an entity from context}

% For every entity $e$ in the sentence, paragraph and section context around the target entity $e_T$, we derive a distribution over co-occurring entities $e'$
% with $e$ using the frequency of co-occurrence and the relatedness of co-occurring entities to $e$. We may treat this distribution as the conditional probability distribution $P(e' \vert e)$. To find the co-occurring entities, we use the Entity Context Document (ECD) \cite{chatterjee2019why} for $e$. The ECD is created by first retrieving a candidate set of passages using $e$ as the query and then concatenating all passages which mention $e$.
% We score an aspect $A$ in following ways.

% \begin{enumerate}
%     \item \textbf{Simple Frequency Distribution (SF-Dist).}
    
%     We score aspects using frequently co-occurring entities with an entity $e$ from the sentence, paragraph and section context around the target entity, which also occur in the aspect content. Formally,:
%     \begin{equation}
%     \label{eq:score-aspect-using-simple-freq-dist}
%         \text{Score(A)} = \sum_{e' \in A}P(e' \vert e)
%     \end{equation}
%     where $e'$ is an entity in the aspect $A$ which also frequently co-occurs with $e$, $e$ is an entity from the context around the target entity, and $P( e' \vert e)$ is the frequency distribution over co-occurring entities.
    
%     \item \textbf{Weighted Frequency Distribution (WF-Dist).} 
    
%     Our intuition is that co-occurring entities which are more related to the target entity should contribute more to the final aspect scores. To investigate how relatedness of co-occurring entities affects the aspect scores, we experiment with scoring an aspect $A$ using a weighted sum of frequencies of entities $e'$ in the aspect, weighted by the relatedness of $e'$ to the target entity $e_T$. Formally,
%     \begin{equation}
%         \label{eq:score-aspect-using-weighted-freq-dist}
%         \text{Score(A)} = \sum_{e' \in A} \text{Relatedness}(e', e_T) \times P(e' \vert e)
%     \end{equation}
%     where $e'$ is an entity in the aspect $A$ which also frequently co-occurs with $e$, $e_T$ is the target entity, $e$ is an entity from the context around the target entity $e_T$, and $\text{Relatedness}(e', e_T)$ is found using WAT \cite{piccinno2014wat}.

The method of Nanni et al. uses an exact matches of contextual entities $e'$ in the context and aspect content. However since there are only few such entities, we explore methods to derive an expanded set of prominent entities $e'$, then rank aspects by how frequently they match these most prominent entities $e'$. 

For every entity $e$ in the context of the target entity $e_T$ (sentence, paragraph or section), we retrieve top $k$ passages from a background corpus using $e$'s name as a query. (For the evaluation we arbitrarily chose $k=100$, results of other contexts will be made available in an online appendix). We construct a bag of potential expansion entities from all entities mentioned in all $E \times k$ passages that were retrieved through $E$ queries, one for each contextual entity $e$. We preserve the frequency $\text{tf}_{e}(e')$ with which entities occur in the passage ranking for entity $e$. 
%
From this frequency, we derive a prominence $P(e'|e_T)$ and use it to reweigh entities in this bag. The prominence incorporates both the degree of association of $e'$ with the contextual entity $e$, but also how consistent $e'$ is associated with other entities $e$ that occur in the context we seek to link. Akin to language models, the term frequency $\text{tf}_{e}(e')$ is normalized to sum to one over all contains entities $e''$.

    \begin{equation}
    \label{eq:prominence}
        P(e' \vert e_T) =  \frac{1}{E} \sum_e  \sum_{e'} \frac{\text{tf}_{e}(e')}{\sum_{e''}\text{tf}_{e}(e'')}
    \end{equation}



Finally, we match prominent entities $e'$ in target aspects $A$: the more frequent  prominent entities $e'$ are mentioned in the aspect content, the higher the aspect is ranked. 

    \begin{equation}
    \label{eq:score-aspect-using-simple-freq-dist}
        \text{Score(A)} = \sum_{e' \in A}P(e' \vert e_T)
    \end{equation}



We explore three variants of this approach.

\begin{enumerate}
     \item \textbf{Simple Frequency Distribution (SF-Dist).}
     Using the prominence score $P(e' \vert e_T)$ produced via passage ranking of all entities $e$ mentioned in the context.

This co-occurrence-based entity scoring metric allows us to match more entities $e'$ in the aspect than solely using entities in the context $e$. However, as any expansion approach, it is prone to promote spurious matches. 
 
 \item \textbf{Weighted Frequency Distribution (WF-Dist).} 
 To remedy the promotion of spurious matches, we combine the prominence score with an entity-relatedness score between entities $e'$ and the target entity $e_T$.
 
 \begin{equation}
         \label{eq:score-aspect-using-weighted-freq-dist}
        \text{Score(A)} = \sum_{e' \in A} \text{Relatedness}(e', e_T) \cdot P(e' \vert e_T)
    \end{equation}


\item \textbf{Relatedness Distribution (Rel-Dist).} 
Same approach as in Equation  \ref{eq:score-aspect-using-weighted-freq-dist} but the prominence score is ignored. We only use the prominence approach to select a set of entities $e'$. 

\end{enumerate}

\subsubsection{Co-occurring entities with the target entity} 
\label{subsubsec:Co-occurring entities with the target entity}

Since contextual entities $e$ might be less well suited than the target entity $e_T$, we explore several methods that emphasize a connection to the target entity.



%We use two sources of co-occurring entities with the target entity $e_T$: Entity Context Document (\textbf{ECD}) for $e_T$ and the Wikipedia page (\textbf{Wiki}) of $e_T$. We then score an aspect using the co-occurring entities with $e_T$ in two ways.

    \ld{do you have a naming convention to distinguish methods from 4.3.1 (contextual) from these here (focused on target entity)? }
\begin{enumerate}
    \item \textbf{Rel-Dist-ECD.} Using the prominence approach from Section \ref{subsubsec:Co-occurring entities with an entity from context}, but building an entity set only in for the target entity, while ignoring other contextual entities $e$.
    \item \textbf{Rel-Dist-Wiki.} Instead of identifying entities through other passages, we consider entities $e'$ mentioned on the Wikipedia page of the target entity $e_T$. These entities $e'$ are ranked by relatedness to $e_T$.
\end{enumerate}

Instead of ranking aspects $A$ by a sum of prominence/relatedness scores that they contain, we invert the approach and rank aspects by retrieval models. The query is formed by the title of the target entity which expanded with top 20 entities $e'$ --- these are weighted in the query akin to RM3 \cite{lavrenko2001relevance}. We explore weighted combinations of retrieval models as implemented in Lucene. BM25 (default parameters), Language Models with Jelinek-Mercer (LMJM) smoothing $(\lambda = 0.4)$, and Language Models with Dirichlet (LMDS) smoothing, with both RM1 and RM3 expansion models on entities.


    
%    from Equation  \ref{eq:score-aspect-using-simple-freq-dist} with either frequency distribution \textbf{(SF-Dist-ECD)} or relatedness distribution \textbf{(Rel-Dist-ECD/Rel-Dist-Wiki)} over co-occurring entities to score an aspect.
\begin{enumerate}        
    \item \textbf{RS-Asp-Freq-ECD.} Expand weighted by prominence score.
    \item \textbf{RS-Asp-Rel-ECD.} Expand by relatedness score.
    \item \textbf{RS-Asp-Rel-Wiki.} Expand with entities on Wikipedia page of $e_T$, top 20 by relatedness scores.
\end{enumerate}




 \subsection{Joint Aspect Linking}\label{jordan-co-entity}
In relation to joint entity linking, we can produce aspect links for contextual entities $e$, then use an aspect-to-aspect similarity function to improve the aspect link prediction for the target entity $e_T$. One advantage of this approach is that it provides a more specific context in which to compare two entities. For example, consider a sentence like ``Make sure that you keep chocolate away from your dogs.'' There is no indication that this has anything to do with health, but if we first linked the ``Dogs'' entity mention, we would see that the content of the ``Health'' aspect explicitly discusses the danger of Theobromine poisoning from chocolate, and this is the only context in which chocolate is discussed on the Wikipedia page of  ``Dogs''. It becomes apparent that the relevant aspect in Chocolate is ``Health effects'', because it discusses Theobromine poisoning. Had we instead linked the ``Chocolate'' entity mention without using contextual entities, then a section that mentions the word ``chocolate'' multiple times may have been the most likely candidate for aspect linking. In the following we describe an approach to take advantage of this kind of reasoning.

%\ld{refer back to entity relatedness section, but please don't bash the other part of this work. I usually phrase it as a hypothesis. We conjecture that ... -- be careful that each of these hypotheses is followed up with experimental evidence}

%\ld{Just because entities are contained in the context, does not mean that they are relevant for linking the aspect}

For the majority of contextial entities $e$, we we do not have access to the ground truth aspect in our benchmark. Nevertheless supervised learning is possible: We can apply a preliminary aspect linker to contextual entities, and then use their linked aspects as a potential indicator for determining the correct aspect to link to the target entity.
In this paper, we use context-specific features to score the relevance of aspects contained in contextual entities.
By using aspect-to-aspect similarity features, we can control how the relevance of aspects in the target entity are influenced by the relevance of aspects in contextual entities. 

\subsubsection{Aspect-to-Aspect Similarities}
We use the following aspect-to-aspect similarity functions to measure the similarity between aspects in contextual entities and aspects in the target entity.

\textbf{Unigram Overlap.} The number of unigrams that overlap based on the text contained in each entity aspect.

\textbf{Character n-gram Overlap.} We use a sliding window to generate character 4-grams from each entity aspect text, and the similarity between to entity aspects is respect to the overlap of these 4-grams.

\textbf{Header Overlap.}
We also use the Character n-gram Overlap method with respect to the section header of each entity aspect.

\textbf{Entity Overlap.}
Each entity aspect also contains a number of entity links within the main body of its text. We count the overlap of these entities with respect to two entity aspects.

\textbf{Word2Vec Cosine Similarity.} 
We embed the text of each entity aspect by using Word2Vec to generate word embeddings, and then we average these embeddings together. We measure the cosine similarity between two entities with respect to these embeddings.

\smallskip

\subsubsection{Joint Aspect Linking.}

We score the relevance of an contextual entity aspect $a_{ij}$ for the target aspect $a$ using a supervised aspect-to-aspect similarity function $sim$, which combines the similarity features described above.  
Let $a$ be an entity aspect, $E$ be the collection of co-entities for a given paragraph context, $A_i$ be the set of aspects of contextual entities $e_i$. By marginalizing over contextual entities and their aspects, we can score the aspect $a$ of the target entity $e_T$ in a context-specific manner.

\begin{equation}
P(a) = \frac{1}{E} \sum_{e_i  \in E}\sum_{a_ij \in A_{e_i}}  P(a_{ij} \vert e_i) \dot \text{sim}(a_{ij}, a) \cdot 
\end{equation}

\subsubsection{Supervised aspect-to-aspect similarity learning}
We learn the supervised aspect-to-aspect similarity from low-level similarity feature functions $sim_k$, defined above. These are combined with a linear model using parameters $\lambda_k$. We use a pipeline training  paradigm, where we first train an non-joint aspect-linker $\tilde{P}(a_{ij} \vert e_i)$ on the set of features $\mathcal{F}$ detailed in Sections  \ref{sec:Approach}.1--\ref{subsubsec:Co-occurring entities with an entity from context}, which is used to determine contextual aspects as a probability distribution. In a separate step, we train the joint aspect linker $P(a \vert e_T)$ on a set of features that augments $\mathcal{F}$ with additional aspect-to-aspect features $\mathcal{A}$. To make the training efficient, we exploit the linearity of $\frac{1}{E}\sum_{e_i} \sum_{a_{ij}} \tilde{P}(a_{ij}|e_i) \sum_k sim_k(a,a_{ij}) \lambda_k = \sum_k \lambda_k \mathcal{A}_k$ to derive joint-aspect features as $\mathcal{A}_k=\frac{1}{E}\sum_{e_i} \sum_{a_{ij}} \tilde{P}(a_{ij}|e_i) sim_k(a,a_{ij})$.



\ld{make Shubam's $A$ lowercase $a$}


\begin{comment}
We give a brief overview of the same again for reference. 

Nanni et al.\cite{nanni2018entity} consider three types of aspect representations for finding its similarity to the entity mention in context. 

\textbf{Header.} Rank aspects based on similarity of the mention in context to the header of each section in the Wikipedia page.

\textbf{Content.} Rank aspects based on the similarity between the mention in context and the content of each section of the Wikipedia page of the entity.

\textbf{Entity.} Overlap of entities mentioned in the context of the entity mention and the content of a section on the Wikipedia page of the entity.

They use the following features to rank aspects using the representations above.

\textbf{TF-IDF.} Cosine similarity between the TF-IDF (logarithmic, L2-normalized) vector of contextual mention and aspect.

\textbf{BM25.} Rank aspect representations using the contextual mention as a query using BM25 ($k_1=2, b=0.75)$.

\textbf{Word Embeddings.} Cosine similarity between the mention in context and
the aspect using pre-trained GloVe \cite{pennington2014glove} embeddings of dimension 300. 

\textbf{Entity Embeddings.} Use 500 dimensional RDF2Vec \cite{ristoski2016rdf2vec} embeddings to embed entities in the context of the entity mention and a section from the Wikipedia page of the entity, then compute
the document vector using the TF-IDF of an an entity in context of the entity mention and its embedding.
\end{comment}