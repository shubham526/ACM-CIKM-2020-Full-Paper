\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}\PackageWarning{TODO:}{#1!}}

\section{Evaluation}
\label{sec:Evaluation}
In this section, a quantitative evaluation of our system is presented on the entity-aspect dataset from Nanni et al.\cite{nanni2018entity}. We begin by presenting some research questions pertaining to three broad components of our system: Entity Salience, Entity Relatedness and Co-occurring entities, which our experiments aim to address(Section \ref{subsec:Research Questions}). We then describe our experimental settings (Section \ref{subsec:Evaluation Paradigm}), followed by the baselines (Section \ref{subsec:Baselines}). We end this section with a discussion of our experiments and results (Section \ref{subsec:Results}).

\subsection{Research Questions}
\label{subsec:Research Questions}

\begin{itemize}
\item[\textbf{RQ1}] Does entity salience affect the task? If yes, then to what extent?
\item[\textbf{RQ2}] Does entity relatedness affect the task? If yes, then to what extent? 
\item[\textbf{RQ3}] Is the frequency or relatedness of co-occurring entities a better indicator of a good aspect?
\item[\textbf{RQ4}] Can we use co-occurring entities to infer the correct entity aspect?
\item[\textbf{RQ5}] Does filtering out non-relevant co-occurring entities improve results when inferring the correct entity aspect?
\end{itemize}

\subsection{Evaluation Paradigm}
\label{subsec:Evaluation Paradigm}

\paragraph{\textbf{Datasets.}} Due to the lack of other datasets in this area, we use the Entity Aspect Linking dataset from Nanni et al.\cite{nanni2018entity}. It consists of 201 entity mentions from Wikipedia along with their sentence, paragraph and section context and a list of candidate aspects for the mention. We use this dataset for training a L2R algorithm using 5-fold cross validation. 

We use the TREC Complex Answer Retrieval (CAR) track \cite{dietz2018trec}\footnote{\url{http://trec-car.cs.unh.edu}} dataset as a source of passages when building the Entity Context Document in Section \ref{subsubsec:Entity relatedness based features}. It consists of an entity linked corpus consisting of paragraphs from the entire English Wikipedia. 

Since neural networks require large training data to perform well and the dataset from Nanni et al. \cite{nanni2018entity} contains only 201 entity mention, we construct our own training data using the \textit{AllButBenchmark} from TREC CAR. \textit{AllButBenchmark} consists of nearly whole of Wikipedia (all pages and all sections on those pages along with the images embedded on the page), except for pages which were included in the official evaluation topics for years during which the track was active at TREC.

Our new training data consists of 88,000 training examples extracted from the page data in \textit{AllButBenchmark}. It was constructed by retaining entity links within Wikipedia pages that link to top-level sections in other Wikipedia pages, after filtering out for irrelevant sections (such as ``References'', ``See Also'', etc.). We did not filter out self-referential links (a link to another section in the same page). The dataset is modelled after Nanni et al.\cite{nanni2018entity}. Each example contains the sentence context, paragraph context, entity mention and candidate aspects. The candidate aspects are the top level sections of the Wikipedia page of the entity linked to the mention. Each candidate aspect contains information about its section header, text, and entities that are contained within it.


%For each example in which an entity in a paragraph links to a section in a page, the candidate aspects for the example are the top level sections where the ground truth is the section that was linked to. Each example contains the sentence context, paragraph context and entity mention that contains the link to a different section. Each candidate represents a section, and contains information about its section header, text, and entities that are contained within it.
    
%These consist of entity links within Wikipedia pages that link to top-level sections in other Wikipedia pages, after filtering out for irrelevant sections (such as ``References'', ``See Also'', etc.). We did not filter out self-referential links (a link to another section in the same page). 
    


\paragraph{\textbf{Ground Truth.}} The dataset from Nanni et al. \cite{nanni2018entity} contains a ground truth file which maps a mention to the correct aspect. %The TREC Complex Answer Retrieval track \cite{dietz2018trec} dataset contains both passage and entity ground truth data. 
The ground truth for our new training data was constructed in a similar way: the true aspect of an entity mention is the top-level section on the Wikipedia page of the entity linked to the mention and in which the mention appears.

\paragraph{\textbf{Evaluation Metrics.}} We use Precision at 1 (P@1) and Mean Reciprocal Rank (MRR) as our evaluation metrics.

\paragraph{\textbf{Entity Linking.}} Both data sets contain the entities from Wikipedia mentioned in the context and aspect content. In addition, we use WAT \cite{piccinno2014wat} to annotate the context and aspect content to get more entities. 

\paragraph{\textbf{Entity Salience Detection.}} We use SWAT \cite{swat}  to find the salience of an entity in text. Given some text, SWAT outputs the entities along with their salience scores in the text. For example, using the online demo \footnote{https://swat.d4science.org/}, given the two passages in Figure \ref{fig:Salience}, SWAT correctly predicts \textit{Boris Johnson} as salient in Passage 2 (Score = 0.6), and non-salient in Passage 1 (Score = 0.15). 

\paragraph{\textbf{Entity Relatedness Detection.}} We use the Entity Relatedness system from WAT \cite{piccinno2014wat} to find relatedness between pairs of entities. Given a list of entities, WAT provides the relatedness measure between every pair of entities in the list. For example, given the entity list consisting of \textit{Boris Johnson}, \textit{Theresa May} and \textit{Donald Trump}, WAT predicts the relatedness between every pair of entities as follows:
\begin{quote}
    (\text{Boris Johnson}, \text{Donald Trump}) = 0.37, \\
    (\text{Theresa May},\text{Donald Trump})    = 0.38, \\
    (\text{Boris Johnson}, \text{Theresa May})  = 0.67
\end{quote}

\paragraph{\textbf{Machine Learning.}}
We apply our methods to produce an aspect ranking for every entity mention. We then treat each ranking as a feature and perform 5-fold cross validation with a listwise learning-to-rank (L2R) method (Coordinate Ascent) optimized for Precision at 1 (P@1). We use RankLib\footnote{Dang, V. "The Lemur Project-Wiki-RankLib." Lemur Project,[Online]. Available: \url{http://sourceforge. net/p/lemur/wiki/RankLib}.} for this purpose. 


\subsection{Baselines}
\label{subsec:Baselines}

\textbf{Baseline 1: Nanni et al.} We re-implement all features from Nanni et al. \cite{nanni2018entity} and use a supervised combination of sentence, paragraph and section context features as our baselines. \\
\textbf{Baseline 2: Size.} We consider the length of each section (in number of tokens) and link the entity-mention to the longest. \\
%\textbf{Baseline 3: Content Overlap.} Overlap between the tokens in the sentence, paragraph and section context of the mention. \\
%\textbf{Baseline 4: Entity Overlap.} Overlap between the entities in the sentence, paragraph and section context of the mention. \\

\subsection{Results}
\label{subsec:Results}
The most interesting results are presented in Table \ref{tab:Results}. Due to space constraints other results were moved to the online appendix for this paper. Below, we discuss each of the research questions presented in Section \ref{subsec:Research Questions}.
%Below, we discuss results from our experiments with respect to the three broad components of our system: entity salience, entity relatedness and co-occurring entities, and answer the research questions presented in Section \ref{subsec:Research Questions}.


\begin{figure}
    \centering
    \includegraphics [scale=0.5]{plot-cropped.png}
    \caption{Difficulty-test for P@1, comparing Nanni et al.(Sentence) to various L2R systems.}
    \label{fig:difficulty-plot}
\end{figure}

\begin{table}[]
\caption{Performance of individual entity rankings (in terms of MAP) on different contexts obtained using frequency and relatedness and combined with L2R.}
\label{tab:Results-Entity-Rankings-Freq-And-Rel}
\begin{tabular}{@{}llll@{}}
 \toprule
            & Sentence & Paragraph & Section \\ \midrule
Frequency   & 0.04     & 0.06      & 0.13    \\
Relatedness & 0.03     & 0.03      & 0.04    \\ \midrule
L2R         & 0.01     & 0.02      & 0.03   \\
 \bottomrule
\end{tabular}
\end{table}

\begin{comment}
\begin{table}[t]
    \caption{Performance of individual entity rankings and combined with L2R.}
    \label{tab:Results-Entity-Rankings-Freq-And-Rel}
    %\scalebox{0.9}{
    \begin{tabular}{@{}lllll@{}}
        \toprule
        Method & MAP & P@R & MRR \\ \midrule
        
        Frequency (Sentence) & 0.04 & 0.04 & 0.93 \\
        
        Frequency (Paragraph) & 0.06 & 0.06 & 0.87 \\
        
        Frequency (Section) & 0.13 & 0.14 & 0.95 \\
        
        L2R (Frequency) & 0.04 & 0.06 & 0.59 \\
        
        \midrule
        
        Relatedness (Sentence) & 0.03 & 0.04 & 0.75 \\
        
        Relatedness (Paragraph) & 0.03 & 0.05 & 0.71 \\
        
        Relatedness (Section) & 0.04 & 0.09 & 0.64 \\
        
        L2R (Relatedness) & 0.02 & 0.05 & 0.44 \\
        
        \midrule
        L2R (All) & 0.04 & 0.06 & 0.60 \\
         
         \bottomrule
    \end{tabular}
    %}
\end{table}
\end{comment}


\begin{table}[t]
\caption{Performance of individual entity rankings (in terms of MAP) on different contexts obtained using entity salience and combined with L2R.}
\label{tab:Results-Entity-Rankings-Sal}
\begin{tabular}{@{}llll@{}}
\toprule
                                & Sentence & Paragraph & Section \\ \midrule
Context-Salient-Content-Salient & 0.009    & 0.001     & 0.008   \\
Context-Salient-Content-All     & 0.002    & 0.005     & 0.003   \\
Context-All-Content-Salient     & 0.006    & 0.008     & 0.003   \\
Context-All-Content-All         & 0.02     & 0.02      & 0.01    \\ \midrule
L2R                             & 0.006    & 0.009     & 0.005   \\ \bottomrule
\end{tabular}
\end{table}


\begin{table}[t]
    \caption{Performance with standard error of individual features and combined with L2R, including subsets/ablations.}
    \label{tab:Results}
    %\scalebox{0.9}{
    \begin{tabular}{@{}lllll@{}}
        \toprule
        Method & 
       P@1 &
        MRR \\ 
        
        \midrule
        
         Nanni et al. (Sentence) \cite{nanni2018entity}&  
      0.70$\pm$0.03&
      0.81$\pm$0.02
      \\
      
      
    
      Nanni et al. (Paragraph) \cite{nanni2018entity}&  
      0.65$\pm$0.03&
      0.78$\pm$0.03
      \\
      
      Nanni et al. (Section) \cite{nanni2018entity}&  
      0.57$\pm$0.03&
      0.73$\pm$0.03
      \\
      
      \midrule
        
     
      
     
      Nanni et al. (Sentence) (Re-implemented)&  
      0.67$\pm$0.03&
      0.79$\pm$0.02
      \\
      
      
    
      Nanni et al. (Paragraph) (Re-implemented)&  
      0.64$\pm$0.03&
      0.78$\pm$0.03
      \\
      
      Nanni et al. (Section) (Re-implemented)&  
      0.53$\pm$0.03&
      0.71$\pm$0.03
      \\
      
     
      Size &
      0.39$\pm$0.03&
      0.60$\pm$0.03
      \\
      
      \midrule
      
     
    Sal-EM   &   
      0.19$\pm$0.03 &
      0.46$\pm$0.03
      \\
      
      
      
      SEC (Sentence)  &    
      0.23$\pm$0.03 &
      0.53$\pm$0.03
      \\
      
      
      AEC (Sentence)   &    
      0.51$\pm$0.03 &
      0.70$\pm$0.03
      \\
       \midrule
      
     
      SF-Dist (Sentence)   &    
      0.54$\pm$0.03 &
      0.72$\pm$0.03
      \\
      
     
      WF-Dist (Sentence)  &    
      0.49$\pm$0.03 &
      0.67$\pm$0.03
      \\
      
      
      Rel-Dist (Sentence)  &    
      0.43$\pm$0.03 &
     0.62$\pm$0.03
      \\
       \midrule
      
    
      SF-Dist-ECD   &    
      0.35$\pm$0.03 &
      0.59$\pm$0.03
      \\
      
      
      Rel-Dist-ECD  &    
      0.36$\pm$0.03 &
      0.60$\pm$0.03
      \\
      
     
      RS-Asp-Freq-ECD (LMJM + RM1)  &     
      0.35$\pm$0.03 &
      0.59$\pm$0.03
      \\
      
       
      RS-Asp-Rel-ECD (LMJM + RM1) &     
      0.40$\pm$0.03 &
      0.60$\pm$0.03
      \\
       \midrule
      
      
      Rel-Dist-Wiki  &    
      0.37$\pm$0.03 &
      0.60$\pm$0.03
      \\
      
      
       
      RS-Asp-Rel-Wiki (BM25 + RM3)  &     
      0.39$\pm$0.03 &
      0.61$\pm$0.03
      \\
      \midrule
      
       Subset-1 (Only Relatedness) &
      0.48$\pm$0.03 &
      0.68$\pm$0.03
      \\
      
      
      Subset-2 (Only Salience) &
      0.59$\pm$0.03 &
      0.72$\pm$0.03
      \\
      
       Subset-3 (Rel. + Lex. + Sem.) &
      0.66$\pm$0.03 &
      0.78$\pm$0.03
      \\
      
       Subset-4 (Sal + Lex. + Sem.) &
      
      0.72$\pm$0.03 &
      0.82$\pm$0.03
      \\
      
      
      Subset-5 (Sal. + Rel. + Lex. + Sem.) &
      0.70$\pm$0.03 &
      0.81$\pm$0.03
      \\
     
      
      

     
      
       \bottomrule
    \end{tabular}
    %}
\end{table}

\input{results-shubham} 

\input{results-jordan}


  